"""
ASR Inference Script using Facebook Wav2Vec2 Base (960h)

- Model: facebook/wav2vec2-base-960h
- Source: https://huggingface.co/facebook/wav2vec2-base-960h
- Description: 
    - This script performs automatic speech recognition (ASR) on pre-synthesized audio files (TTS outputs) using the pretrained wav2vec ASR model.
    - It loads URLs pointing to audio files generated from different TTS systems (chatterbox, kokoro, melo, etc.), transcribes them, and appends the results to their respective DataFrames.
- DataFrame: 
    - The DataFrame contains both the original text and the file paths of the WAV files generated by the TTS model.
    - Ex) df_kokoro contains the original text along with the file paths of the WAV files generated from that text. 
    
- Output: The transcriptions are saved into new columns named 'wav2vec' and exported as CSV files for further evaluation (e.g., WER/CER calculation).
"""
# Import necessary libraries
from transformers import Wav2Vec2Processor, Wav2Vec2ForCTC
from datasets import load_dataset
import torch
import pandas as pd
import librosa
from tqdm import tqdm

# Load pretrained processor and model from HuggingFace
processor = Wav2Vec2Processor.from_pretrained("facebook/wav2vec2-base-960h")
model = Wav2Vec2ForCTC.from_pretrained("facebook/wav2vec2-base-960h")

# Function to perform ASR inference on a given audio file
def make_asr_script(url):
    # Load audio from file and resample to 16kHz
    speech_array, sampling_rate = librosa.load(url, sr=16000)

    # Tokenize input
    inputs = processor(speech_array, return_tensors="pt", sampling_rate=16000)

    # Run inference (no gradient computation needed)
    with torch.no_grad():
        logits = model(**inputs).logits

    # Get predicted token IDs and decode them to text
    predicted_ids = torch.argmax(logits, dim=-1)
    transcription = processor.batch_decode(predicted_ids)[0].strip()

    return transcription


# === ASR Inference: Chatterbox ===
df_chatterbox = pd.read_csv("")

scription_list = []
for i in tqdm(range(df_chatterbox.shape[0])):
    temp_url = df_chatterbox.iloc[i, 3]
    scription_temp = make_asr_script(temp_url)
    scription_list.append(scription_temp)

if df_chatterbox.shape[0] == len(scription_list):
    df_chatterbox['wav2vec'] = scription_list


# === ASR Inference: Kokoro ===
df_kokoro = pd.read_csv("")

scription_list = []
for i in tqdm(range(df_kokoro.shape[0])):
    temp_url = df_kokoro.iloc[i, 3]
    scription_temp = make_asr_script(temp_url)
    scription_list.append(scription_temp)

if df_kokoro.shape[0] == len(scription_list):
    df_kokoro['wav2vec'] = scription_list


# === ASR Inference: Melo ===
df_melo = pd.read_csv("")

scription_list = []
for i in tqdm(range(df_melo.shape[0])):
    temp_url = df_melo.iloc[i, 3]
    scription_temp = make_asr_script(temp_url)
    scription_list.append(scription_temp)

if df_melo.shape[0] == len(scription_list):
    df_melo['wav2vec'] = scription_list


# === ASR Inference: Outetts ===
df_outetts = pd.read_csv("")
scription_list = []
for i in tqdm(range(df_outetts.shape[0])):
    temp_url = df_outetts.iloc[i, 1]
    scription_temp = make_asr_script(temp_url)
    scription_list.append(scription_temp)
if df_outetts.shape[0] == len(scription_list):
    df_outetts['wav2vec'] = scription_list


# === Save transcriptions to CSV ===
df_chatterbox.to_csv("", index=False, encoding="utf-8-sig")
df_kokoro.to_csv("", index=False, encoding="utf-8-sig")
df_outetts.to_csv("", index=False, encoding="utf-8-sig")
df_melo.to_csv("", index=False, encoding="utf-8-sig")
